{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d456b50",
   "metadata": {},
   "source": [
    "### **CS481 Notebook 14**: Part of speech tagging with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be1eaba",
   "metadata": {},
   "source": [
    "Let's import nltk package first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a03bcbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766f75bc",
   "metadata": {},
   "source": [
    "Let's download the Punkt Tokenizer (it may already be downloaded). We will use it for tokenization, of course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed9a92d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dzikjac\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f94108",
   "metadata": {},
   "source": [
    "Punkt Tokenizer is downloaded and ready to use. What is it exactly, though? It is an unsupervised sentence boundary detection algorithm **that you can train yourself**. NLTK has a **pre-trained** implementation. Let's set it up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "debe932e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.punkt.PunktSentenceTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665d997d",
   "metadata": {},
   "source": [
    "We will also need a tagger. Let's load one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e2d8256c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\dzikjac\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa383b4",
   "metadata": {},
   "source": [
    "Here's our input text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6c47dd4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = 'The futuristic special effects are top notch. Sadly the degree of characterisation necessary to power a fairly static storyline is missing. With limited action, performances need to be riveting in themselves. Imagine Anthony Hopkins as Hari Seldon. Tilda Swinton as Demerzel. Maybe a Mel Gibson as Cleon. These are people that can hold the screen with the slightest word or gesture. Foundation onscreen needs such characterisation as the plot itself is quite dry and action limited for the sake of concept.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae115414",
   "metadata": {},
   "source": [
    "Let's segment input text into individual sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7d252e7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences = nltk.tokenize.sent_tokenize(text, language = 'english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918a0b21",
   "metadata": {},
   "source": [
    "... and print them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "67ead588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The futuristic special effects are top notch.\n",
      "Sadly the degree of characterisation necessary to power a fairly static storyline is missing.\n",
      "With limited action, performances need to be riveting in themselves.\n",
      "Imagine Anthony Hopkins as Hari Seldon.\n",
      "Tilda Swinton as Demerzel.\n",
      "Maybe a Mel Gibson as Cleon.\n",
      "These are people that can hold the screen with the slightest word or gesture.\n",
      "Foundation onscreen needs such characterisation as the plot itself is quite dry and action limited for the sake of concept.\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9591963",
   "metadata": {},
   "source": [
    "Now that we have all sentences ready, we can tokenize each one and then use the POS tagger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "28877bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('futuristic', 'JJ'), ('special', 'JJ'), ('effects', 'NNS'), ('are', 'VBP'), ('top', 'JJ'), ('notch', 'NN'), ('.', '.')]\n",
      "[('Sadly', 'RB'), ('the', 'DT'), ('degree', 'NN'), ('of', 'IN'), ('characterisation', 'NN'), ('necessary', 'JJ'), ('to', 'TO'), ('power', 'NN'), ('a', 'DT'), ('fairly', 'RB'), ('static', 'JJ'), ('storyline', 'NN'), ('is', 'VBZ'), ('missing', 'VBG'), ('.', '.')]\n",
      "[('With', 'IN'), ('limited', 'JJ'), ('action', 'NN'), (',', ','), ('performances', 'NNS'), ('need', 'VBP'), ('to', 'TO'), ('be', 'VB'), ('riveting', 'VBG'), ('in', 'IN'), ('themselves', 'PRP'), ('.', '.')]\n",
      "[('Imagine', 'NNP'), ('Anthony', 'NNP'), ('Hopkins', 'NNP'), ('as', 'IN'), ('Hari', 'NNP'), ('Seldon', 'NNP'), ('.', '.')]\n",
      "[('Tilda', 'NNP'), ('Swinton', 'NNP'), ('as', 'IN'), ('Demerzel', 'NNP'), ('.', '.')]\n",
      "[('Maybe', 'RB'), ('a', 'DT'), ('Mel', 'NNP'), ('Gibson', 'NNP'), ('as', 'IN'), ('Cleon', 'NNP'), ('.', '.')]\n",
      "[('These', 'DT'), ('are', 'VBP'), ('people', 'NNS'), ('that', 'WDT'), ('can', 'MD'), ('hold', 'VB'), ('the', 'DT'), ('screen', 'NN'), ('with', 'IN'), ('the', 'DT'), ('slightest', 'JJS'), ('word', 'NN'), ('or', 'CC'), ('gesture', 'NN'), ('.', '.')]\n",
      "[('Foundation', 'NNP'), ('onscreen', 'PRP'), ('needs', 'VBZ'), ('such', 'JJ'), ('characterisation', 'NN'), ('as', 'IN'), ('the', 'DT'), ('plot', 'NN'), ('itself', 'PRP'), ('is', 'VBZ'), ('quite', 'RB'), ('dry', 'JJ'), ('and', 'CC'), ('action', 'NN'), ('limited', 'VBN'), ('for', 'IN'), ('the', 'DT'), ('sake', 'NN'), ('of', 'IN'), ('concept', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# If you want to also remove stop words, uncomment this line below:\n",
    "#stopWords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "for sentence in sentences:\n",
    "    # Let's tokenize the sentence\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    # You could remove stop words if you wanted to\n",
    "    #words = [w for w in words if not w in stop_words]\n",
    " \n",
    "    # let's use the Part of Speech tagger to tag each word\n",
    "    taggedWords = nltk.pos_tag(words)\n",
    " \n",
    "    print(taggedWords)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
