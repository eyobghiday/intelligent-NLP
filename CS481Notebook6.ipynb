{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "093b6477",
   "metadata": {},
   "source": [
    "**CS481 Notebook 6:** Text normalization with NTLK (based on RegEx) - simple sentence tokenization example\n",
    "source: https://stackoverflow.com/questions/35118596/python-regular-expression-not-working-properly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a9d810",
   "metadata": {},
   "source": [
    "Import NLTK package first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d60a872e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7b1dd4",
   "metadata": {},
   "source": [
    "Set up a string variable for the text you want to process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42abf730",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'That U.S.A. poster-print costs $12.40...'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c835cce",
   "metadata": {},
   "source": [
    "Specify regular expression pattern for splitting / tokenization (note: this may require modification for other re and nltk versions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "46fb8059",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"\"\"(?x)                   # set flag to allow verbose regexps\n",
    "              (?:[A-Z]\\.)+           # abbreviations, e.g. U.S.A.\n",
    "              |\\d+(?:\\.\\d+)?%?       # numbers, incl. currency and percentages\n",
    "              |\\w+(?:[-']\\w+)*       # words w/ optional internal hyphens/apostrophe\n",
    "              |(?:[+/\\-@&*])         # special characters with meanings\n",
    "            \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7103e6c5",
   "metadata": {},
   "source": [
    "Now, set up the NLTK tokenizer (RegEx-based):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78fb4d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.regexp.RegexpTokenizer(pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6104d75",
   "metadata": {},
   "source": [
    "Tokenization time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9548afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cddcd79",
   "metadata": {},
   "source": [
    "Let's print out all the tokens after tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4804746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That\n",
      "U.S.A.\n",
      "poster-print\n",
      "costs\n",
      "12.40\n"
     ]
    }
   ],
   "source": [
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4c3f11",
   "metadata": {},
   "source": [
    "We are ready for some stemming or lemmatization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
